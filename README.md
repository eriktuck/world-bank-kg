[![DOI](https://zenodo.org/badge/eriktuck/world-bank-kg.svg)](https://zenodo.org/badge/latestdoi/eriktuck/world-bank-kg)
# Overview
**world-bank-kg** is a demo repo for a *Graph Retrieval-Augmented Generation* (GraphRAG) framework for question answering with the World Bank Open Knowledge Repository (OKR). This project explores approaches to promote Semantic Interoperability (SI) within and across humanitarian aid and international development organizations, including 
1. **Semantic graph integration:** Introduces a graph layer based on the Resource Description Framework (RDF) to represent entities, relationships, and document metadata in a standards-compliant and machine-readable format, enabling connection with the broader Semantic Web.
2. **Ontology and taxonomy alignment:** Extends beyond keyword- and embedding-based retrieval by linking extracted entities to established taxonomies and ontologies such as Wikidata, UNBIS Thesaurus, and the SDG Taxonomy for semantic interoperability promoting cross-institutional reuse and alignment.
3. **Knowledge preservation:** Demonstrates how graph-based AI pipelines can preserve, structure, and synthesize institutional knowledge, supporting long-term discoverability and learning.

# Environment
This project uses [uv](https://docs.astral.sh/uv/) for fast and reproducible Python environments. Install `uv` if you don't already have it.

After cloning this repo, use `uv` to recreate the environment from the `pyproject.toml` file:

```bash
uv sync
```

This command automatically creates a virtual environment (if one doesnâ€™t already exist) and installs all core and development dependencies, including required `spaCy` models `en_core_web_sm` and `en_core_sci_sm`.

This project expects Ollama to be running on port `11434` with `llama3.2:latest` available for LLM inference. You can verify this by running:

```bash
ollama pull llama3.2:latest
ollama serve
```

Model configuration (e.g., model name, port, or provider) can be adjusted in the code where LLM inference is defined.

To use frontier models (e.g., ChatGPT), please configure your API key in `./secrets/.env`.

# Data
Data for this project is read directly from the [World Bank API](https://documents.worldbank.org/en/publication/documents-reports/api).

# Workflow
Run the `main.py` script to execute the full data-processing and knowledge-graph pipeline.

```bash
uv run python -m main
```

During execution, the following directories will be created automatically to store intermediate outputs:
- `./logs/`: Application logs. The default log level is `DEBUG`, which can be adjusted in the configuration or code.
- `./output/`: Parsed PDF files generated by MinerU.
- `./storage/`: Persisted LlamaIndex document storage and metadata. 

The final knowledge graph is serialized in Turtle (`.ttl`) format and saved to the project root directory.

If needed, you can rebuild the index from previously generated outputs using:

```bash
uv run python -m scripts.rebuild_from_files
```

# Testing
This project uses [pytest](https://docs.pytest.org/en/stable/) for automated testing. All tests are located in the `./tests/` directory.

To run the full test suite from project root:

```bash
uv run pytest -v -s
```
